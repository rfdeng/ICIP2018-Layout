\section{RESULTS}
\label{sec:Res}

We evaluate our method on two widely used dataset: Hedau's dataset \cite{hedau2009recovering} and LSUN dataset.
\subsection{LSUN Results}
\label{sec:LSUN}
We train our multi-channel FCN on the relabeled LSUN dataset released by \cite{ren2016coarse}. The dataset consists of 4000 training, 394 validation, and 1000 testing images. We extract geometric hints from original images and resize all the images, depth and normal maps to $321\times321$ using bicubic interpolation. Then these three types of data are integrated to train the ResNet101 based multi-channel FCN. We evaluate our results using the official toolkit which provide two standard metrics: pixelwise error and corner error. The pixelwise error is computed by counting the percentage of pixels that are mismatched. (The Hungarian algorithm is applied to address the labeling ambiguity problem) The corner error is computed by calculate the Euclidean distance between predicted corners and corresponding ground truth corners.

Our performance on LSUN test set compared with other methods is shown in Table~\ref{table:comparison-lsun}. ... 

\begin{table}
	\centering 
	\caption{Performance benchmarking on the LSUN dataset}
	\label{table:comparison-lsun}
	\begin{tabular}{l|c|c}
		\hline 
		Method & Corner Error (\%) & Pixel Error (\%) \\
		\hline
		Hedau et al. (2009)~\cite{hedau2009recovering} & 15.48 & 24.23 \\
		Mallya et al. (2015)~\cite{mallya2015learning} & 11.02 & 16.71 \\
		Zhang et al. (2017)~\cite{zhang2017learning} & 8.70 & 12.49 \\
		Dasgupta et al. (2016)~\cite{dasgupta2016delay} & 8.20 & 10.63 \\
		Ren et al. (2016)~\cite{ren2016three} & 7.95 & 9.31 \\
		Proposed FCN-GF & xxx & 9.51 \\
		\hline
	\end{tabular}
\end{table}

\subsection{Hedau Results}
\label{sec:Hedau}
We also conduct experiment on a relatively smaller dataset published by Hedau \emph{et al}. \cite{hedau2009recovering}, which consists of 209 training images and 104 testing images. We directly predict the semantic surface on Hedau's test set using the model trained on LSUN. Pixel error is adopted to evaluate the results, see Table~\ref{table:comparison-hedau}. ...


\begin{table}
	\centering 
	\caption{Performance benchmarking on Hedau's dataset}
	\label{table:comparison-hedau}
	\begin{tabular}{l|c}
		\hline 
		Method & Pixel Error (\%) \\
		\hline
		Hedau et al. (2009)~\cite{hedau2009recovering} & 21.20 \\
		Mallya et al. (2015)~\cite{mallya2015learning} & 12.83 \\
		Zhang et al. (2017)~\cite{zhang2017learning} & 12.7 \\
		Dasgupta et al. (2016)~\cite{dasgupta2016delay} & 9.73 \\
		Ren et al. (2016)~\cite{ren2016three} & 8.67 \\
		Zhao et al. (2017)~\cite{zhao2017physics} & 6.60 \\
		Proposed MC-FCN & xxx \\
		\hline
	\end{tabular}
\end{table}


\subsection{Analysis of Geometric Hints}
\label{sec:ablation}
To explore the benefits of geometric hints for semantic surface segmentation, we train different FCNs with or without geometric information as additional input. And in order to compare with \cite{ren2016coarse}, we use the FCN architecture based on VGG16. Qualitative results on LSUN validation set are demonstrated by Fig.~\ref{fig:fcn-comparison}. Intuitively, the geometric hints help to remove the spurious regions caused by clutter and generate more accurate semantic segmentation. Quantitive results are shown in Table~\ref{table:ablation}. 

\begin{table}
	\centering
	\caption{Pixelwise accuracy for surface label prediction.}
	\label{table:ablation}
	\begin{tabular}{c|c}
		\hline
		Network & Accuracy\\
		\hline
		FCN-32s & 0.8109 \\ 
		MC-FCN  & 0.8392 \\
		Ren et al. (2016)~\cite{ren2016three} & xxx \\
		\hline
	\end{tabular}
	
\end{table}


